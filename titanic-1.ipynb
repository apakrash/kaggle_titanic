{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for printing full numpy array\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "#for printing till 3 decimal places\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accounting for cabins\n",
    "\n",
    "df1_c=pd.read_csv('trainCabin.csv')\n",
    "df2_c=df1_c[['Pclass','Sex','Age','SibSp','Parch','Fare', 'Embarked','Survived','Cabin']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_t1=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
       "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n",
       "       'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#complete Ignore PassengerId,Name,\n",
    "\n",
    "#maybe ticket,cabin\n",
    "\n",
    "df2=df1[['Pclass','Sex','Age','SibSp','Parch','Fare', 'Embarked','Survived']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_t2=df_t1[['Pclass','Sex','Age','SibSp','Parch','Fare', 'Embarked']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nage mean\\nembark would be interesting\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "age mean\n",
    "embark would be interesting\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_t3=df_t2.dropna(axis=0)\n",
    "df_t3=df_t2.fillna(df_t2.mean()['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_t=df_t3.iloc[:,0:7].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#one hot encoding\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 6] = labelencoder_X_2.fit_transform(X[:, 6])\n",
    "\n",
    "\n",
    "labelencoder_X_t1 = LabelEncoder()\n",
    "X_t[:, 1] = labelencoder_X_t1.fit_transform(X_t[:, 1])\n",
    "\n",
    "labelencoder_X_t2 = LabelEncoder()\n",
    "X_t[:, 6] = labelencoder_X_t2.fit_transform(X_t[:, 6])\n",
    "\n",
    "onehotencoder = OneHotEncoder(categorical_features = [6])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "#avoiding the dummy variable trap\n",
    "X = X[:, 1:]\n",
    "\n",
    "onehotencoder_t = OneHotEncoder(categorical_features = [6])\n",
    "X_t = onehotencoder_t.fit_transform(X_t).toarray()\n",
    "#avoiding the dummy variable trap\n",
    "X_t = X_t[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from sklearn import preprocessing, cross_validation, neighbors\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.ensemble import ExtraTreesClassifier\n",
    "#from sklearn import svm\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "#from sklearn import linear_model\n",
    "#from sklearn.linear_model import SGDClassifier\n",
    "#from sknn.mlp import Classifier, Layer\n",
    "import numpy as np\n",
    "import pickle\n",
    "#from sklearn import linear_model\n",
    "#from sklearn.feature_selection import SelectKBest\n",
    "#from sklearn.feature_selection import chi2\n",
    "#from sklearn import linear_model\n",
    "#from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias is: 0.799648506151\n",
      "Variance is: 0.573426573427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apakrash\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 100.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    }
   ],
   "source": [
    "#logistic regression 0.2 test set\n",
    "\n",
    "#train train split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\n",
    "\n",
    "#normalization\n",
    "\n",
    "#question, will fitting a numpy array to pandas cause any issue\n",
    "\n",
    "rob_scaler=sklearn.preprocessing.RobustScaler()\n",
    "rob_scaled=rob_scaler.fit(X)\n",
    "x_rob_scaled=rob_scaled.transform(X_train)\n",
    "df_rob_normalized=pd.DataFrame(x_rob_scaled)\n",
    "Ip=df_rob_normalized.values\n",
    "Op=y_train\n",
    "\n",
    "### training\n",
    "\n",
    "logreg=LogisticRegression(C=1.5,random_state=1,n_jobs=100)\n",
    "logreg.fit(Ip,Op)\n",
    "bias=logreg.score(Ip,Op)\n",
    "variance=logreg.score(X_test,y_test)\n",
    "print(\"Bias is: \"+ str(bias))\n",
    "print(\"Variance is: \"+ str(variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias is: 0.823293172691\n",
      "Variance is: 0.593457943925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apakrash\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 100.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    }
   ],
   "source": [
    "#logistic regression 0.2 test set\n",
    "\n",
    "#train train split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=0)\n",
    "\n",
    "#normalization\n",
    "\n",
    "#question, will fitting a numpy array to pandas cause any issue\n",
    "\n",
    "rob_scaler=sklearn.preprocessing.RobustScaler()\n",
    "rob_scaled=rob_scaler.fit(X)\n",
    "x_rob_scaled=rob_scaled.transform(X_train)\n",
    "df_rob_normalized=pd.DataFrame(x_rob_scaled)\n",
    "Ip=df_rob_normalized.values\n",
    "Op=y_train\n",
    "\n",
    "\n",
    "\n",
    "### training\n",
    "\n",
    "logreg=LogisticRegression(C=1.5,random_state=1,n_jobs=100)\n",
    "logreg.fit(Ip,Op)\n",
    "bias=logreg.score(Ip,Op)\n",
    "variance=logreg.score(X_test,y_test)\n",
    "print(\"Bias is: \"+ str(bias))\n",
    "print(\"Variance is: \"+ str(variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## final test set normalization\n",
    "\n",
    "x_rob_scaled_t=rob_scaled.transform(X_t)\n",
    "df_rob_normalized_t=pd.DataFrame(x_rob_scaled_t)\n",
    "Ip_t=df_rob_normalized_t.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias is: 0.993975903614\n",
      "Variance is: 0.757009345794\n"
     ]
    }
   ],
   "source": [
    "#randomForest\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=0)\n",
    "rob_scaler=sklearn.preprocessing.RobustScaler()\n",
    "rob_scaled=rob_scaler.fit(X_train)\n",
    "x_rob_scaled=rob_scaled.transform(X_train)\n",
    "df_rob_normalized=pd.DataFrame(x_rob_scaled)\n",
    "Ip=df_rob_normalized.values\n",
    "Op=y_train\n",
    "\n",
    "x_test_normalized=rob_scaled.transform(X_test)\n",
    "x_test_rob_normalized_df=pd.DataFrame(x_test_normalized)\n",
    "x_test_values=x_test_rob_normalized_df.values\n",
    "\n",
    "rf=RandomForestClassifier(n_estimators=125,max_depth=36,random_state=1)\n",
    "rf.fit(Ip,Op)\n",
    "bias=rf.score(Ip,Op)\n",
    "variance=rf.score(x_test_values,y_test)\n",
    "print(\"Bias is: \"+ str(bias))\n",
    "print(\"Variance is: \"+ str(variance))\n",
    "#y_pred=rf.predict(x_test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trees 100\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.757009345794\n",
      "trees 200\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.752336448598\n",
      "trees 300\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.747663551402\n",
      "trees 400\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.742990654206\n",
      "trees 500\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.757009345794\n",
      "trees 600\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.752336448598\n",
      "trees 700\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.761682242991\n",
      "trees 800\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.761682242991\n",
      "trees 900\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.766355140187\n"
     ]
    }
   ],
   "source": [
    "#playing with number of trees\n",
    "\n",
    "#trees  100 to 1000\n",
    "\n",
    "for i in range(100,1000,100):\n",
    "    print('trees '+str(i))\n",
    "    rf=RandomForestClassifier(n_estimators=i,max_depth=36,random_state=1)\n",
    "    rf.fit(Ip,Op)\n",
    "    bias=rf.score(Ip,Op)\n",
    "    variance=rf.score(x_test_values,y_test)\n",
    "    print(\"\\tBias is: \"+ str(bias))\n",
    "    print(\"\\tVariance is: \"+ str(variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trees 1000\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.761682242991\n",
      "trees 1100\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.766355140187\n",
      "trees 1200\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.766355140187\n",
      "trees 1300\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.761682242991\n",
      "trees 1400\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.761682242991\n",
      "trees 1500\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.761682242991\n",
      "trees 1600\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.761682242991\n",
      "trees 1700\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.761682242991\n",
      "trees 1800\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.761682242991\n",
      "trees 1900\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.761682242991\n"
     ]
    }
   ],
   "source": [
    "#playing with number of trees\n",
    "\n",
    "#trees n 1000 to 2000\n",
    "\n",
    "for i in range(1000,2000,100):\n",
    "    print('trees '+str(i))\n",
    "    rf=RandomForestClassifier(n_estimators=i,max_depth=36,random_state=1)\n",
    "    rf.fit(Ip,Op)\n",
    "    bias=rf.score(Ip,Op)\n",
    "    variance=rf.score(x_test_values,y_test)\n",
    "    print(\"\\tBias is: \"+ str(bias))\n",
    "    print(\"\\tVariance is: \"+ str(variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.766355140187\n"
     ]
    }
   ],
   "source": [
    "#tree 900 is the best\n",
    "\n",
    "rf=RandomForestClassifier(n_estimators=900,max_depth=36,random_state=1)\n",
    "rf.fit(Ip,Op)\n",
    "bias=rf.score(Ip,Op)\n",
    "variance=rf.score(x_test_values,y_test)\n",
    "print(\"\\tBias is: \"+ str(bias))\n",
    "print(\"\\tVariance is: \"+ str(variance))\n",
    "y_rf=rf.predict(Ip_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trees 900\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.761682242991\n",
      "trees 910\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.757009345794\n",
      "trees 920\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.757009345794\n",
      "trees 930\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.757009345794\n",
      "trees 940\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.761682242991\n",
      "trees 950\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.761682242991\n",
      "trees 960\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.757009345794\n",
      "trees 970\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.757009345794\n",
      "trees 980\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.757009345794\n",
      "trees 990\n",
      "\tBias is: 0.993975903614\n",
      "\tVariance is: 0.761682242991\n"
     ]
    }
   ],
   "source": [
    "#playing with number of trees\n",
    "\n",
    "#trees n 900 to 1000 give best results\n",
    "\n",
    "for n_estimators in range(900,1000,10):\n",
    "    print('trees '+str(n_estimators))\n",
    "    rf=RandomForestClassifier(n_estimators,max_depth=36,random_state=1)\n",
    "    rf.fit(Ip,Op)\n",
    "    bias=rf.score(Ip,Op)\n",
    "    variance=rf.score(x_test_values,y_test)\n",
    "    print(\"\\tBias is: \"+ str(bias))\n",
    "    print(\"\\tVariance is: \"+ str(variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 900 trees give best answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias is: 0.781124497992\n",
      "Variance is: 0.584112149533\n"
     ]
    }
   ],
   "source": [
    "# random forest with AdaBoost, bad result\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=0)\n",
    "\n",
    "#normalization\n",
    "\n",
    "#question, will fitting a numpy array to pandas cause any issue\n",
    "\n",
    "rob_scaler=sklearn.preprocessing.RobustScaler()\n",
    "rob_scaled=rob_scaler.fit(X)\n",
    "x_rob_scaled=rob_scaled.transform(X_train)\n",
    "df_rob_normalized=pd.DataFrame(x_rob_scaled)\n",
    "Ip=df_rob_normalized.values\n",
    "Op=y_train\n",
    "\n",
    "x_test_normalized=rob_scaled.transform(X_test)\n",
    "x_test_rob_normalized_df=pd.DataFrame(x_test_normalized)\n",
    "x_test_values=x_test_rob_normalized_df.values\n",
    "\n",
    "\n",
    "rf=RandomForestClassifier(n_estimators=900,max_depth=40, oob_score = \"TRUE\", n_jobs = 5,random_state =50 ,max_features = 'sqrt', min_samples_leaf = 50)\n",
    "#rf.fit(Ip,Op)\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=100, base_estimator=rf,learning_rate=0.001)\n",
    "clf.fit(Ip,Op)\n",
    "bias=clf.score(Ip,Op)\n",
    "variance=clf.score(X_test,y_test)\n",
    "print(\"Bias is: \"+ str(bias))\n",
    "print(\"Variance is: \"+ str(variance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 8)\n",
      "(712,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias is: 0.796133567663\n",
      "Variance is: 0.811188811189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apakrash\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#mlpc\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\n",
    "rob_scaler=sklearn.preprocessing.RobustScaler()\n",
    "rob_scaled=rob_scaler.fit(X_train)\n",
    "x_rob_scaled=rob_scaled.transform(X_train)\n",
    "df_rob_normalized=pd.DataFrame(x_rob_scaled)\n",
    "Ip=df_rob_normalized.values\n",
    "Op=y_train\n",
    "\n",
    "x_test_normalized=rob_scaled.transform(X_test)\n",
    "x_test_rob_normalized_df=pd.DataFrame(x_test_normalized)\n",
    "x_test_values=x_test_rob_normalized_df.values\n",
    "\n",
    "mlpc= MLPClassifier(solver='adam', alpha=1e-5,hidden_layer_sizes=(4,5), random_state=1)\n",
    "mlpc.fit(Ip,Op)\n",
    "bias=mlpc.score(Ip,Op)\n",
    "variance=mlpc.score(x_test_values,y_test)\n",
    "print(\"Bias is: \"+ str(bias))\n",
    "print(\"Variance is: \"+ str(variance))\n",
    "y_pred=mlpc.predict(x_test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-175-57c5b46652b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,1\n",
      "Bias is: 0.601054481547\n",
      "Variance is: 0.573426573427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apakrash\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2\n",
      "Bias is: 0.398945518453\n",
      "Variance is: 0.426573426573\n",
      "1,3\n",
      "Bias is: 0.797891036907\n",
      "Variance is: 0.804195804196\n",
      "1,4\n",
      "Bias is: 0.601054481547\n",
      "Variance is: 0.573426573427\n",
      "1,5\n",
      "Bias is: 0.81546572935\n",
      "Variance is: 0.832167832168\n",
      "1,6\n",
      "Bias is: 0.81546572935\n",
      "Variance is: 0.839160839161\n",
      "1,7\n",
      "Bias is: 0.813708260105\n",
      "Variance is: 0.832167832168\n",
      "1,8\n",
      "Bias is: 0.810193321617\n",
      "Variance is: 0.825174825175\n",
      "1,9\n",
      "Bias is: 0.806678383128\n",
      "Variance is: 0.825174825175\n",
      "2,1\n",
      "Bias is: 0.599297012302\n",
      "Variance is: 0.573426573427\n",
      "2,2\n",
      "Bias is: 0.748681898067\n",
      "Variance is: 0.748251748252\n",
      "2,3\n",
      "Bias is: 0.787346221441\n",
      "Variance is: 0.79020979021\n",
      "2,4\n",
      "Bias is: 0.796133567663\n",
      "Variance is: 0.797202797203\n",
      "2,5\n",
      "Bias is: 0.796133567663\n",
      "Variance is: 0.769230769231\n",
      "2,6\n",
      "Bias is: 0.797891036907\n",
      "Variance is: 0.811188811189\n",
      "2,7\n",
      "Bias is: 0.773286467487\n",
      "Variance is: 0.804195804196\n",
      "2,8\n",
      "Bias is: 0.782073813708\n",
      "Variance is: 0.741258741259\n",
      "2,9\n",
      "Bias is: 0.79086115993\n",
      "Variance is: 0.783216783217\n",
      "3,1\n",
      "Bias is: 0.456942003515\n",
      "Variance is: 0.447552447552\n",
      "3,2\n",
      "Bias is: 0.787346221441\n",
      "Variance is: 0.741258741259\n",
      "3,3\n",
      "Bias is: 0.80316344464\n",
      "Variance is: 0.783216783217\n",
      "3,4\n",
      "Bias is: 0.752196836555\n",
      "Variance is: 0.713286713287\n",
      "3,5\n",
      "Bias is: 0.782073813708\n",
      "Variance is: 0.762237762238\n",
      "3,6\n",
      "Bias is: 0.799648506151\n",
      "Variance is: 0.783216783217\n",
      "3,7\n",
      "Bias is: 0.74165202109\n",
      "Variance is: 0.748251748252\n",
      "3,8\n",
      "Bias is: 0.794376098418\n",
      "Variance is: 0.783216783217\n",
      "3,9\n",
      "Bias is: 0.787346221441\n",
      "Variance is: 0.762237762238\n",
      "4,1\n",
      "Bias is: 0.697715289982\n",
      "Variance is: 0.706293706294\n",
      "4,2\n",
      "Bias is: 0.797891036907\n",
      "Variance is: 0.839160839161\n",
      "4,3\n",
      "Bias is: 0.732864674868\n",
      "Variance is: 0.706293706294\n",
      "4,4\n",
      "Bias is: 0.710017574692\n",
      "Variance is: 0.65034965035\n",
      "4,5\n",
      "Bias is: 0.796133567663\n",
      "Variance is: 0.811188811189\n",
      "4,6\n",
      "Bias is: 0.808435852373\n",
      "Variance is: 0.804195804196\n",
      "4,7\n",
      "Bias is: 0.760984182777\n",
      "Variance is: 0.713286713287\n",
      "4,8\n",
      "Bias is: 0.77855887522\n",
      "Variance is: 0.748251748252\n",
      "4,9\n",
      "Bias is: 0.796133567663\n",
      "Variance is: 0.79020979021\n",
      "5,1\n",
      "Bias is: 0.601054481547\n",
      "Variance is: 0.573426573427\n",
      "5,2\n",
      "Bias is: 0.398945518453\n",
      "Variance is: 0.426573426573\n",
      "5,3\n",
      "Bias is: 0.808435852373\n",
      "Variance is: 0.797202797203\n",
      "5,4\n",
      "Bias is: 0.804920913884\n",
      "Variance is: 0.832167832168\n",
      "5,5\n",
      "Bias is: 0.79086115993\n",
      "Variance is: 0.804195804196\n",
      "5,6\n",
      "Bias is: 0.833040421793\n",
      "Variance is: 0.804195804196\n",
      "5,7\n",
      "Bias is: 0.811950790861\n",
      "Variance is: 0.811188811189\n",
      "5,8\n",
      "Bias is: 0.801405975395\n",
      "Variance is: 0.811188811189\n",
      "5,9\n",
      "Bias is: 0.813708260105\n",
      "Variance is: 0.797202797203\n",
      "6,1\n",
      "Bias is: 0.398945518453\n",
      "Variance is: 0.426573426573\n",
      "6,2\n",
      "Bias is: 0.398945518453\n",
      "Variance is: 0.426573426573\n",
      "6,3\n",
      "Bias is: 0.80316344464\n",
      "Variance is: 0.741258741259\n",
      "6,4\n",
      "Bias is: 0.808435852373\n",
      "Variance is: 0.797202797203\n",
      "6,5\n",
      "Bias is: 0.810193321617\n",
      "Variance is: 0.811188811189\n",
      "6,6\n",
      "Bias is: 0.792618629174\n",
      "Variance is: 0.769230769231\n",
      "6,7\n",
      "Bias is: 0.818980667838\n",
      "Variance is: 0.797202797203\n",
      "6,8\n",
      "Bias is: 0.804920913884\n",
      "Variance is: 0.748251748252\n",
      "6,9\n",
      "Bias is: 0.80316344464\n",
      "Variance is: 0.797202797203\n",
      "7,1\n",
      "Bias is: 0.601054481547\n",
      "Variance is: 0.573426573427\n",
      "7,2\n",
      "Bias is: 0.810193321617\n",
      "Variance is: 0.811188811189\n",
      "7,3\n",
      "Bias is: 0.811950790861\n",
      "Variance is: 0.797202797203\n",
      "7,4\n",
      "Bias is: 0.801405975395\n",
      "Variance is: 0.832167832168\n",
      "7,5\n",
      "Bias is: 0.826010544815\n",
      "Variance is: 0.79020979021\n",
      "7,6\n",
      "Bias is: 0.813708260105\n",
      "Variance is: 0.804195804196\n",
      "7,7\n",
      "Bias is: 0.822495606327\n",
      "Variance is: 0.832167832168\n",
      "7,8\n",
      "Bias is: 0.817223198594\n",
      "Variance is: 0.825174825175\n",
      "7,9\n",
      "Bias is: 0.820738137083\n",
      "Variance is: 0.818181818182\n",
      "8,1\n",
      "Bias is: 0.398945518453\n",
      "Variance is: 0.426573426573\n",
      "8,2\n",
      "Bias is: 0.820738137083\n",
      "Variance is: 0.825174825175\n",
      "8,3\n",
      "Bias is: 0.80316344464\n",
      "Variance is: 0.839160839161\n",
      "8,4\n",
      "Bias is: 0.801405975395\n",
      "Variance is: 0.783216783217\n",
      "8,5\n",
      "Bias is: 0.813708260105\n",
      "Variance is: 0.825174825175\n",
      "8,6\n",
      "Bias is: 0.797891036907\n",
      "Variance is: 0.811188811189\n",
      "8,7\n",
      "Bias is: 0.81546572935\n",
      "Variance is: 0.825174825175\n",
      "8,8\n",
      "Bias is: 0.806678383128\n",
      "Variance is: 0.811188811189\n",
      "8,9\n",
      "Bias is: 0.808435852373\n",
      "Variance is: 0.825174825175\n",
      "9,1\n",
      "Bias is: 0.813708260105\n",
      "Variance is: 0.776223776224\n",
      "9,2\n",
      "Bias is: 0.834797891037\n",
      "Variance is: 0.804195804196\n",
      "9,3\n",
      "Bias is: 0.820738137083\n",
      "Variance is: 0.804195804196\n",
      "9,4\n",
      "Bias is: 0.820738137083\n",
      "Variance is: 0.832167832168\n",
      "9,5\n",
      "Bias is: 0.836555360281\n",
      "Variance is: 0.811188811189\n",
      "9,6\n",
      "Bias is: 0.820738137083\n",
      "Variance is: 0.832167832168\n",
      "9,7\n",
      "Bias is: 0.836555360281\n",
      "Variance is: 0.825174825175\n",
      "9,8\n",
      "Bias is: 0.834797891037\n",
      "Variance is: 0.818181818182\n",
      "9,9\n",
      "Bias is: 0.847100175747\n",
      "Variance is: 0.832167832168\n"
     ]
    }
   ],
   "source": [
    "for a in range(1,10,1):\n",
    "    for b in range(1,10,1):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\n",
    "        rob_scaler=sklearn.preprocessing.RobustScaler()\n",
    "        rob_scaled=rob_scaler.fit(X_train)\n",
    "        x_rob_scaled=rob_scaled.transform(X_train)\n",
    "        df_rob_normalized=pd.DataFrame(x_rob_scaled)\n",
    "        Ip=df_rob_normalized.values\n",
    "        Op=y_train\n",
    "\n",
    "        x_test_normalized=rob_scaled.transform(X_test)\n",
    "        x_test_rob_normalized_df=pd.DataFrame(x_test_normalized)\n",
    "        x_test_values=x_test_rob_normalized_df.values\n",
    "\n",
    "        mlpc= MLPClassifier(solver='adam', alpha=1e-5,hidden_layer_sizes=(a,b), random_state=1)\n",
    "        mlpc.fit(Ip,Op)\n",
    "        bias=mlpc.score(Ip,Op)\n",
    "        variance=mlpc.score(x_test_values,y_test)\n",
    "        print(str(a)+','+str(b))\n",
    "        print(\"Bias is: \"+ str(bias))\n",
    "        print(\"Variance is: \"+ str(variance))\n",
    "        #y_pred=mlpc.predict(x_test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#8,3 variance 0.839160839161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias is: 0.80316344464\n",
      "Variance is: 0.839160839161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apakrash\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#using 8,3\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\n",
    "rob_scaler=sklearn.preprocessing.RobustScaler()\n",
    "rob_scaled=rob_scaler.fit(X_train)\n",
    "x_rob_scaled=rob_scaled.transform(X_train)\n",
    "df_rob_normalized=pd.DataFrame(x_rob_scaled)\n",
    "Ip=df_rob_normalized.values\n",
    "Op=y_train\n",
    "\n",
    "x_test_normalized=rob_scaled.transform(X_test)\n",
    "x_test_rob_normalized_df=pd.DataFrame(x_test_normalized)\n",
    "x_test_values=x_test_rob_normalized_df.values\n",
    "\n",
    "\n",
    "\n",
    "mlpc= MLPClassifier(solver='adam', alpha=1e-5,hidden_layer_sizes=(8,3), random_state=1)\n",
    "mlpc.fit(Ip,Op)\n",
    "bias=mlpc.score(Ip,Op)\n",
    "variance=mlpc.score(x_test_values,y_test)\n",
    "print(\"Bias is: \"+ str(bias))\n",
    "print(\"Variance is: \"+ str(variance))\n",
    "y_mlpc=mlpc.predict(Ip_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\t0.778631764597\n",
      "200\t0.775104427736\n",
      "300\t0.76455026455\n",
      "400\t0.759296389121\n",
      "500\t0.757523438225\n",
      "600\t0.75226028033\n",
      "700\t0.75226028033\n",
      "800\t0.748751508401\n",
      "900\t0.74698783997\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "for i in range(100,1000,100):\n",
    "    clf = AdaBoostClassifier(n_estimators=i)\n",
    "    scores = cross_val_score(clf, Ip,Op)\n",
    "    print(str(i)+'\\t'+str(scores.mean())) \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\t0.76455026455\n",
      "Bias is: 0.866432337434\n",
      "Variance is: 0.769230769231\n"
     ]
    }
   ],
   "source": [
    "adaclf = AdaBoostClassifier(n_estimators=300)\n",
    "scores = cross_val_score(adaclf, Ip,Op)\n",
    "print(str(i)+'\\t'+str(scores.mean())) \n",
    "   \n",
    "\n",
    "\n",
    "adaclf.fit(Ip,Op)\n",
    "bias=adaclf.score(Ip,Op)\n",
    "variance=adaclf.score(x_test_values,y_test)\n",
    "print(\"Bias is: \"+ str(bias))\n",
    "print(\"Variance is: \"+ str(variance))\n",
    "y_ada=adaclf.predict(Ip_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AdaBoostClassifier with algorithm='SAMME.R' requires that the weak learner supports the calculation of class probabilities with a predict_proba method.\nPlease change the base estimator or set algorithm='SAMME' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-333-68b31110d6c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madaBoost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-332-cc227032294b>\u001b[0m in \u001b[0;36madaBoost\u001b[1;34m(n_estimators)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0madaBoost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[0;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[0;32m    343\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             return_times=True)\n\u001b[1;32m--> 206\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;31m# Check parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;31m# Clear any previous fit results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_validate_estimator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    422\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'predict_proba'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m                 raise TypeError(\n\u001b[1;32m--> 424\u001b[1;33m                     \u001b[1;34m\"AdaBoostClassifier with algorithm='SAMME.R' requires \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m                     \u001b[1;34m\"that the weak learner supports the calculation of class \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m                     \u001b[1;34m\"probabilities with a predict_proba method.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: AdaBoostClassifier with algorithm='SAMME.R' requires that the weak learner supports the calculation of class probabilities with a predict_proba method.\nPlease change the base estimator or set algorithm='SAMME' instead."
     ]
    }
   ],
   "source": [
    "for i in range(100,1000,100):\n",
    "    print(str(i)+'\\t'+str(adaBoost(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('test_rf.csv',y_rf,fmt='%.0f',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('test_mlpc.csv',y_mlpc,fmt='%.0f',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('test_adaclf.csv',y_ada,fmt='%.0f',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
